[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Welcome to My Data Science Portfolio",
    "section": "",
    "text": "Welcome to my data science portfolio! This site shows my journey learning data science and analytics. It includes projects, code samples, and insights I’ve gained along the way. This portfolio is a work in progress and I will be updating it regularly as I learn new skills and complete new projects.\nAs I continue my studies, I will continue to add more of my finding and insights from different projects I undertake in the statistics and data science field. You can expect to see projects involving data cleaning, visualization, statistical analysis, and machine learning.\n\n\nThis portfolio shows my work learning data science. Each project includes:\n\nMy code with documentation\nVisualizations I created\nWhat I learned and discovered\n\nI built this site using Quarto and host it on GitHub Pages.\n\n\n\n\nProgramming: Python, Pandas for data analysis, NumPy for numerical computing, R for statistics\nVisualization: Creating charts with Matplotlib, Seaborn, and GGplot2; Rshiny for interactive dashboards\nData Collection: Getting data from files, websites, and APIs\nAnalysis: Finding patterns and answering questions with data\n\n\n\n\n\n\nLearn how to set up a K-Nearest-Neighbors algorithm in the context of classifing board game genres.\n\nThanks for visiting! Feel free to explore my projects and see what I’m learning."
  },
  {
    "objectID": "index.html#about-this-portfolio",
    "href": "index.html#about-this-portfolio",
    "title": "Welcome to My Data Science Portfolio",
    "section": "",
    "text": "This portfolio shows my work learning data science. Each project includes:\n\nMy code with documentation\nVisualizations I created\nWhat I learned and discovered\n\nI built this site using Quarto and host it on GitHub Pages."
  },
  {
    "objectID": "index.html#skills-im-learning",
    "href": "index.html#skills-im-learning",
    "title": "Welcome to My Data Science Portfolio",
    "section": "",
    "text": "Programming: Python, Pandas for data analysis, NumPy for numerical computing, R for statistics\nVisualization: Creating charts with Matplotlib, Seaborn, and GGplot2; Rshiny for interactive dashboards\nData Collection: Getting data from files, websites, and APIs\nAnalysis: Finding patterns and answering questions with data"
  },
  {
    "objectID": "index.html#my-projects",
    "href": "index.html#my-projects",
    "title": "Welcome to My Data Science Portfolio",
    "section": "",
    "text": "Learn how to set up a K-Nearest-Neighbors algorithm in the context of classifing board game genres.\n\nThanks for visiting! Feel free to explore my projects and see what I’m learning."
  },
  {
    "objectID": "projects/index.html",
    "href": "projects/index.html",
    "title": "Projects Overview",
    "section": "",
    "text": "Description: Pick a dataset and explore it to discover insights and answer questions.\n\n\n\nDescription: Find an interesting data source, collect the data, and prepare it for analysis.\n\n\n\nDescription: A comprehensive project that shows off my data science skills.",
    "crumbs": [
      "Projects Overview"
    ]
  },
  {
    "objectID": "projects/index.html#all-projects",
    "href": "projects/index.html#all-projects",
    "title": "Projects Overview",
    "section": "",
    "text": "Description: Pick a dataset and explore it to discover insights and answer questions.\n\n\n\nDescription: Find an interesting data source, collect the data, and prepare it for analysis.\n\n\n\nDescription: A comprehensive project that shows off my data science skills.",
    "crumbs": [
      "Projects Overview"
    ]
  },
  {
    "objectID": "projects/eda.html",
    "href": "projects/eda.html",
    "title": "EDA Project",
    "section": "",
    "text": "This is coming down the pipeline. Check again later.",
    "crumbs": [
      "EDA Project"
    ]
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About Me",
    "section": "",
    "text": "I am a senior at BYU majoring Data Science. I plan to graduate in 2027.\nI love working with data to find correlations and patterns in order to make predictions and informed decisions.\nI currently work for the BYU Statistics department as a grader and tutor for various data science courses, and as a research assistant for a professor in the department. In this role I help with the Learning Outcomes Assessment for the department, which involves collecting and analyzing data on student learning outcomes to help improve the curriculum and teaching methods."
  },
  {
    "objectID": "about.html#background",
    "href": "about.html#background",
    "title": "About Me",
    "section": "",
    "text": "Write a brief introduction about yourself here. Include:\n\nYour academic background\nYour interest in data science\nYour career goals\nAny relevant experience or projects"
  },
  {
    "objectID": "about.html#education",
    "href": "about.html#education",
    "title": "About Me",
    "section": "Education",
    "text": "Education\n\n[BS Data Science] - [BYU], [2027]\n[AS Mathmatics and Science] - [Santiago Canyon College], [2024]\nRelevant Coursework: Statistics(Both Sequential and Bayesian), Data Analysis, Programming, Machine Learning, Data Visualization"
  },
  {
    "objectID": "about.html#skills-interests",
    "href": "about.html#skills-interests",
    "title": "About Me",
    "section": "Skills & Interests",
    "text": "Skills & Interests\n\nTechnical Skills\n\nProgramming: Python, R, C++, SQL\nData Analysis: Pandas, NumPy\nVisualization: Matplotlib, Seaborn\nMachine Learning: Scikit-learn\nTools: Jupyter Notebooks, Git/GitHub, Rshiny\n\n\n\nAreas of Interest\n\nI enjoy working with data in various domains, including:\n\nSports analytics(specifically Soccer)\nGaming(both online and physical games)\nEducation"
  },
  {
    "objectID": "about.html#goals",
    "href": "about.html#goals",
    "title": "About Me",
    "section": "Goals",
    "text": "Goals\nDescribe what you hope to achieve through your data science journey:\n\nShort-term learning objectives\nLong-term career aspirations\nTypes of problems you want to solve"
  },
  {
    "objectID": "about.html#contact",
    "href": "about.html#contact",
    "title": "About Me",
    "section": "Contact",
    "text": "Contact\n\nEmail: corbinpchristiansen@gmail.com\nGitHub: github.com/FBIagent42)\nLinkedIn: linkedin.com/in/corbin-christiansen\n\n\nThis portfolio showcases my learning progress and projects completed during my data science studies."
  },
  {
    "objectID": "projects/data-acquisition.html",
    "href": "projects/data-acquisition.html",
    "title": "Data Acquisition Project",
    "section": "",
    "text": "This is coming down the pipeline. Check again later.",
    "crumbs": [
      "Data Acquisition Project"
    ]
  },
  {
    "objectID": "projects/final-project.html",
    "href": "projects/final-project.html",
    "title": "Final Project",
    "section": "",
    "text": "This is coming down the pipeline. Check again later.",
    "crumbs": [
      "Final Project"
    ]
  },
  {
    "objectID": "blog.html",
    "href": "blog.html",
    "title": "",
    "section": "",
    "text": "Code"
  },
  {
    "objectID": "test.html",
    "href": "test.html",
    "title": "",
    "section": "",
    "text": "CodeShow All CodeHide All Code\n\n\n\n\n\n\nCode\nimport pandas as pd\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nfrom sklearn.preprocessing import MultiLabelBinarizer\nfrom sklearn.model_selection import train_test_split, GridSearchCV\nfrom sklearn.multioutput import MultiOutputClassifier\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.metrics import hamming_loss, f1_score\n\n\n\n\nCode\ndata = pd.read_csv('BGG_Data_Set.csv', encoding='ISO-8859-1')\n# Replace NaN values with empty strings\ndata['Mechanics'] = data['Mechanics'].fillna('')\ndata['Domains'] = data['Domains'].fillna('')\n\n# Remove rows where 'Domains' is still empty\ndata = data[data['Domains'] != \"\"]\ndata = data.reset_index(drop=True)\n\n# Split strings into lists and clean them\ndata['Mechanics'] = data['Mechanics'].str.split(', ').apply(lambda x: [item.strip() for item in x if item.strip()])\ndata['Domains'] = data['Domains'].str.split(', ').apply(lambda x: [item.strip() for item in x if item.strip()])\n\nscaler = StandardScaler()\nscaled_numeric = scaler.fit_transform(data[['Min Players', 'Max Players', 'Play Time', 'Min Age', 'Rating Average', 'BGG Rank', 'Complexity Average']])\nscaled_numeric_df = pd.DataFrame(scaled_numeric, columns=['Min Players', 'Max Players', 'Play Time', 'Min Age', 'Rating Average', 'BGG Rank', 'Complexity Average'])\n\n# Binarize the cleaned lists\nmlb = MultiLabelBinarizer()\nattribute_matrix = pd.DataFrame(mlb.fit_transform(data['Mechanics']), columns=mlb.classes_)\ndomain_matrix = pd.DataFrame(mlb.fit_transform(data['Domains']), columns=mlb.classes_)\n\n# Build feature matrix from the same filtered `data`\nfeature_matrix = pd.concat([scaled_numeric_df, attribute_matrix], axis=1)\n\n\n\n\nCode\nx_train, x_test, y_train, y_test = train_test_split(feature_matrix, domain_matrix, test_size=0.2, random_state=42)\n\n\n\n\nCode\nknn = MultiOutputClassifier(KNeighborsClassifier())\n\nparam_grid_knn = {\n    'estimator__n_neighbors': [10,11,12],\n    'estimator__weights': ['uniform', 'distance'],\n    'estimator__metric': ['euclidean', 'manhattan']\n}\nknn_grid = GridSearchCV(knn, param_grid_knn, cv=5)\nknn_grid.fit(x_train, y_train)\nprint(\"Best KNN parameters:\", knn_grid.best_params_)\n\n\n\n\nCode\n\ny_pred = knn_grid.best_estimator_.predict(x_test)\naccuracy = accuracy_score(y_test, y_pred)\nprint(f\"KNN Accuracy: {accuracy}\")\nprint(\"Hamming Loss:\", hamming_loss(y_test, y_pred))\nprint(\"F1 Score (micro):\", f1_score(y_test, y_pred, average='micro'))\nprint(\"F1 Score (macro):\", f1_score(y_test, y_pred, average='macro'))\n\n\nKNN Accuracy: 0.6200294550810015\nHamming Loss: 0.0703240058910162\nF1 Score (micro): 0.729589428975932\nF1 Score (macro): 0.6247459019197275"
  },
  {
    "objectID": "blog.html#introduction",
    "href": "blog.html#introduction",
    "title": "",
    "section": "Introduction",
    "text": "Introduction\nHave you ever had a hobby that you wish you could predict, without having to do any of the work? Maybe you want to be able to predict what sport an athlete plays just based on height, weight, and eye color? Or guess which actor will be cast for a role based on their previous roles? In this tutorial, we’ll walk through how to use the K-Nearest Neighbors (KNN) algorithm to classify data based on the attributes they possess. We will use Board Game data as an example, modeling the type of board game based of attributes like player count, time, and complexity\nThis guide is perfect for Data Science students who want to apply machine learning to real-world data in a reproducible and interpretable way."
  },
  {
    "objectID": "blog.html#what-youll-learn",
    "href": "blog.html#what-youll-learn",
    "title": "",
    "section": "What You’ll Learn",
    "text": "What You’ll Learn\n\nHow to prepare data for modeling\nHow KNN works and why it’s useful for classification\nHow to implement KNN in Python using scikit-learn\nHow to evaluate model performance\nHow to interpret predictions"
  },
  {
    "objectID": "blog.html#step-1-understanding-the-data",
    "href": "blog.html#step-1-understanding-the-data",
    "title": "",
    "section": "Step 1: Understanding the Data",
    "text": "Step 1: Understanding the Data\nWe’ll use a simplified dataset of board games with the following attributes: ‘Min Players’, ‘Max Players’, ‘Play Time’, ‘Min Age’, ‘Rating Average’, ‘BGG Rank’, ‘Complexity Average’\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nName\nMin Players\nMax Players\nPlay Time\nMin Age\nRating Average\nBGG Rank\nComplexity Average\nType\n\n\n\n\nCodenames\n2\n8\n15\n10\n8.52\n20\n1.2\nParty\n\n\nDominion\n2\n4\n30\n13\n5.37\n5\n2.3\nCard Game\n\n\nTwilight Struggle\n2\n2\n180\n13\n2.80\n572\n3.8\nStrategy\n\n\n\n\nNote: Complexity is a rating from 1 (easy) to 5 (complex), sourced from BoardGameGeek\n\nK-Nearest Neighbors is a non-parametric, instance-based learning algorithm. It classifies a new data point based on the majority label of its k closest neighbors in the feature space. This means that if a new board game is entered and of its k closest neighbors, the majority are Strategy games, it will classify this new game as a Strategy game.\n\nThe KNN Formula\nTo compute the distance between two games, we use either Euclidean distance or Manhattan distance:\n\nEuclidean distance\n\\[\nd(x, y) = \\sqrt{\\sum_{i=1}^{n} (x_i - y_i)^2}\n\\] Where: - ( x ) and ( y ) are feature vectors - ( n ) is the number of features\n\n\nManhattan distance\n\\[\nd(x, y) = \\sum{i=1}^{n} |x_i - y_i|\n\\] Where: - ( x ) and ( y ) are feature vectors - ( n ) is the number of features\nWe will also choose how to weight our distances, either Uniform or Distance\nIf we choose Uniform, all of the neighbors will be weighted the same, while if we choose Distance, the neighbors that are closest will have the biggest influence."
  },
  {
    "objectID": "blog.html#step-2-implementing-knn-in-python",
    "href": "blog.html#step-2-implementing-knn-in-python",
    "title": "",
    "section": "Step 2: Implementing KNN in Python",
    "text": "Step 2: Implementing KNN in Python\nHere’s how to build a simple KNN classifier using scikit-learn.\n\nImport Libraries\nWe start by importing the necessary Libraries\n# Import libraries\nimport pandas as pd\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nfrom sklearn.preprocessing import MultiLabelBinarizer\nfrom sklearn.model_selection import train_test_split, GridSearchCV\nfrom sklearn.multioutput import MultiOutputClassifier\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.metrics import hamming_loss, f1_score\nThese librarys will allow us to access the functions we need to run this KNN algorithm.\n\n\nLoad and Prepare Data\nNext we will load and prepare the data, formatting it a way that allows us to run classification\ndata = pd.read_csv('BGG_Data_Set.csv', encoding='ISO-8859-1') #Replace the inside of the read_csv function with you file\nAfter we import the data we need to decide what to do with the null values, I chose to drop all of the data that wasn’t given a game type. You could also choose to replace null’s with the average or mode of the data, there are many different ways to deal with null data so you choose which works best for your data set\n# Replace NaN values with empty strings\ndata['Mechanics'] = data['Mechanics'].fillna('')\ndata['Domains'] = data['Domains'].fillna('')\n\n# Remove rows where 'Domains' is still empty\ndata = data[data['Domains'] != \"\"]\ndata = data.reset_index(drop=True)\nBecause my data has multiple types of games assigned to each game I had to split them up. First I separated each Domain and Mechanics with ‘,’ and then used “MultiLabelBinarizer” to separate them into different columns with 1 and 0 variables. I also standardized the data to make sure that no one attribute was pulling the algorithm too much. Finally I seperated the data into two data frames, one for the data I am using to train and one with the type of game for each game.\n# Split strings into lists and clean them\ndata['Mechanics'] = data['Mechanics'].str.split(', ').apply(lambda x: [item.strip() for item in x if item.strip()])\ndata['Domains'] = data['Domains'].str.split(', ').apply(lambda x: [item.strip() for item in x if item.strip()])\n\nscaler = StandardScaler()\nscaled_numeric = scaler.fit_transform(data[['Min Players', 'Max Players', 'Play Time', 'Min Age', 'Rating Average', 'BGG Rank', 'Complexity Average']])\nscaled_numeric_df = pd.DataFrame(scaled_numeric, columns=['Min Players', 'Max Players', 'Play Time', 'Min Age', 'Rating Average', 'BGG Rank', 'Complexity Average'])\n\n# Binarize the cleaned lists\nmlb = MultiLabelBinarizer()\nattribute_matrix = pd.DataFrame(mlb.fit_transform(data['Mechanics']), columns=mlb.classes_)\ndomain_matrix = pd.DataFrame(mlb.fit_transform(data['Domains']), columns=mlb.classes_)\n\n# Build feature matrix from the same filtered `data`\nfeature_matrix = pd.concat([scaled_numeric_df, attribute_matrix], axis=1)\n\n\nTrain/Fit Algorithm\nBefore you train your algorithm, you first need to split your data into test and train data. This function “train_test_split” takes a data frame of the attributes and a separate one of the “target” or answers and splits them up randomly.(the test_size parameter allows you to specify how much of the full data you want to be marked test data)\nx_train, x_test, y_train, y_test = train_test_split(feature_matrix, domain_matrix, test_size=0.2, random_state=42)\nThis next part is optional, but if you are not sure which parameters are the best for you data, you can run a grid search to test different combinations. This function will return the combination that performs the best(it may take a while). You can change the values for n_neighbors to values you think might be best. At the end of this function the “knn_grid.fit()” function will fit your model to the data, and now you have a working model\nknn = MultiOutputClassifier(KNeighborsClassifier())\n\nparam_grid_knn = {\n    'estimator__n_neighbors': [10,11,12],\n    'estimator__weights': ['uniform', 'distance'],\n    'estimator__metric': ['euclidean', 'manhattan']\n}\nknn_grid = GridSearchCV(knn, param_grid_knn, cv=5)\nknn_grid.fit(x_train, y_train)\nprint(\"Best KNN parameters:\", knn_grid.best_params_)\n\n\nTest Algorithm\nFinally it is time to test how well our model did. There are various different metrics to test your model and I will show you a few that you can use. Accuracy: This is just the percentage of correct guesses your alogithm made, a simple metric, but usefull useful Hamming loss: This is used specifically for multi-label classification, it measures the percent of labels that are incorrect(either a missed label or a wrong label). A lower score is better.\nF1 Score: This is a combination of both Precision(“Of all the instances my model predicted as positive, how many were actually positive?”) and Recall(“Of all the actual positive instances, how many did my model correctly identify?”)\ny_pred = knn_grid.best_estimator_.predict(x_test)\naccuracy = accuracy_score(y_test, y_pred)\nprint(f\"KNN Accuracy: {accuracy}\")\nprint(\"Hamming Loss:\", hamming_loss(y_test, y_pred))\nprint(\"F1 Score:\", f1_score(y_test, y_pred))\n\n\nUse Model\nNow that our model has been Trained, Fit, and Tested, we can use it on further data. By using the function\n\"model_name\".predict(\"new_data\")\nour model will give us its prediction for this new data point. Using our example we could predict what type a new game is, just based on its mechanical attributes."
  },
  {
    "objectID": "blog.html#conclusion",
    "href": "blog.html#conclusion",
    "title": "",
    "section": "Conclusion",
    "text": "Conclusion\nNow that you have learned how to implement a Multi-Label classification, KNN model, its time to go and try it yourself. Find yourself an interesting data set and see if you can create a KNN prediction model for it. Try out using different attributes or trying to predict different targets and see how the model fairs and use the accuracy scores to see how well you did. Once you are done with this you can go on to learn other prediction algorithms and continue to expand your data science toolkit!"
  },
  {
    "objectID": "about.html#experience-background",
    "href": "about.html#experience-background",
    "title": "About Me",
    "section": "",
    "text": "I am a senior at BYU majoring Data Science. I plan to graduate in 2027.\nI love working with data to find correlations and patterns in order to make predictions and informed decisions.\nI currently work for the BYU Statistics department as a grader and tutor for various data science courses, and as a research assistant for a professor in the department. In this role I help with the Learning Outcomes Assessment for the department, which involves collecting and analyzing data on student learning outcomes to help improve the curriculum and teaching methods."
  }
]