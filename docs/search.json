[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Welcome to My Data Science Portfolio",
    "section": "",
    "text": "Welcome to my data science portfolio! This site shows my journey learning data science and analytics. It includes projects, code samples, and insights I’ve gained along the way. This portfolio is a work in progress and I will be updating it regularly as I learn new skills and complete new projects.\nAs I continue my studies, I will continue to add more of my finding and insights from different projects I undertake in the statistics and data science field. You can expect to see projects involving data cleaning, visualization, statistical analysis, and machine learning.\n\n\nThis portfolio shows my work learning data science. Each project includes:\n\nMy code with documentation\nVisualizations I created\nWhat I learned and discovered\n\nI built this site using Quarto and host it on GitHub Pages.\n\n\n\n\nProgramming: Python, Pandas for data analysis, NumPy for numerical computing, R for statistics\nVisualization: Creating charts with Matplotlib, Seaborn, and GGplot2; Rshiny for interactive dashboards\nData Collection: Getting data from files, websites, and APIs\nAnalysis: Finding patterns and answering questions with data\n\n\n\n\n\n\nLearn how to set up a K-Nearest-Neighbors algorithm in the context of classifing board game genres.\n\nThanks for visiting! Feel free to explore my projects and see what I’m learning."
  },
  {
    "objectID": "index.html#about-this-portfolio",
    "href": "index.html#about-this-portfolio",
    "title": "Welcome to My Data Science Portfolio",
    "section": "",
    "text": "This portfolio shows my work learning data science. Each project includes:\n\nMy code with documentation\nVisualizations I created\nWhat I learned and discovered\n\nI built this site using Quarto and host it on GitHub Pages."
  },
  {
    "objectID": "index.html#skills-im-learning",
    "href": "index.html#skills-im-learning",
    "title": "Welcome to My Data Science Portfolio",
    "section": "",
    "text": "Programming: Python, Pandas for data analysis, NumPy for numerical computing, R for statistics\nVisualization: Creating charts with Matplotlib, Seaborn, and GGplot2; Rshiny for interactive dashboards\nData Collection: Getting data from files, websites, and APIs\nAnalysis: Finding patterns and answering questions with data"
  },
  {
    "objectID": "index.html#my-projects",
    "href": "index.html#my-projects",
    "title": "Welcome to My Data Science Portfolio",
    "section": "",
    "text": "Learn how to set up a K-Nearest-Neighbors algorithm in the context of classifing board game genres.\n\nThanks for visiting! Feel free to explore my projects and see what I’m learning."
  },
  {
    "objectID": "projects/index.html",
    "href": "projects/index.html",
    "title": "Projects Overview",
    "section": "",
    "text": "Description: Pick a dataset and explore it to discover insights and answer questions.\n\n\n\nDescription: Find an interesting data source, collect the data, and prepare it for analysis.\n\n\n\nDescription: A comprehensive project that shows off my data science skills.",
    "crumbs": [
      "Projects Overview"
    ]
  },
  {
    "objectID": "projects/index.html#all-projects",
    "href": "projects/index.html#all-projects",
    "title": "Projects Overview",
    "section": "",
    "text": "Description: Pick a dataset and explore it to discover insights and answer questions.\n\n\n\nDescription: Find an interesting data source, collect the data, and prepare it for analysis.\n\n\n\nDescription: A comprehensive project that shows off my data science skills.",
    "crumbs": [
      "Projects Overview"
    ]
  },
  {
    "objectID": "projects/eda.html",
    "href": "projects/eda.html",
    "title": "EDA Project",
    "section": "",
    "text": "This is coming down the pipeline. Check again later.",
    "crumbs": [
      "EDA Project"
    ]
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About Me",
    "section": "",
    "text": "[BS Data Science] - [BYU], [2027]\n[AS Mathmatics and Science] - [Santiago Canyon College], [2024]\nRelevant Coursework: Statistics(Both Frequentist and Bayesian), Data Analysis, Programming, Machine Learning, Data Visualization\n\n\n\n\n\n\n\nProgramming: Python, R, C++, SQL\nData Analysis: Pandas, NumPy\nVisualization: Matplotlib, Seaborn\nMachine Learning: Scikit-learn\nTools: Jupyter Notebooks, Git/GitHub, Rshiny\n\n\n\n\n\nI enjoy working with data in various domains, including:\n\nSports analytics(specifically Soccer)\nGaming(both online and physical games)\nEducation\n\n\n\n\n\n\n\nEmail: corbinpchristiansen@gmail.com\nGitHub: github.com/FBIagent42)\nLinkedIn: linkedin.com/in/corbin-christiansen\n\n\nThis portfolio showcases my learning progress and projects completed during my data science studies."
  },
  {
    "objectID": "about.html#background",
    "href": "about.html#background",
    "title": "About Me",
    "section": "",
    "text": "Write a brief introduction about yourself here. Include:\n\nYour academic background\nYour interest in data science\nYour career goals\nAny relevant experience or projects"
  },
  {
    "objectID": "about.html#education",
    "href": "about.html#education",
    "title": "About Me",
    "section": "",
    "text": "[BS Data Science] - [BYU], [2027]\n[AS Mathmatics and Science] - [Santiago Canyon College], [2024]\nRelevant Coursework: Statistics(Both Frequentist and Bayesian), Data Analysis, Programming, Machine Learning, Data Visualization"
  },
  {
    "objectID": "about.html#skills-interests",
    "href": "about.html#skills-interests",
    "title": "About Me",
    "section": "",
    "text": "Programming: Python, R, C++, SQL\nData Analysis: Pandas, NumPy\nVisualization: Matplotlib, Seaborn\nMachine Learning: Scikit-learn\nTools: Jupyter Notebooks, Git/GitHub, Rshiny\n\n\n\n\n\nI enjoy working with data in various domains, including:\n\nSports analytics(specifically Soccer)\nGaming(both online and physical games)\nEducation"
  },
  {
    "objectID": "about.html#goals",
    "href": "about.html#goals",
    "title": "About Me",
    "section": "Goals",
    "text": "Goals\nDescribe what you hope to achieve through your data science journey:\n\nShort-term learning objectives\nLong-term career aspirations\nTypes of problems you want to solve"
  },
  {
    "objectID": "about.html#contact",
    "href": "about.html#contact",
    "title": "About Me",
    "section": "",
    "text": "Email: corbinpchristiansen@gmail.com\nGitHub: github.com/FBIagent42)\nLinkedIn: linkedin.com/in/corbin-christiansen\n\n\nThis portfolio showcases my learning progress and projects completed during my data science studies."
  },
  {
    "objectID": "projects/data-acquisition.html",
    "href": "projects/data-acquisition.html",
    "title": "Data Acquisition Project",
    "section": "",
    "text": "This is coming down the pipeline. Check again later.",
    "crumbs": [
      "Data Acquisition Project"
    ]
  },
  {
    "objectID": "projects/final-project.html",
    "href": "projects/final-project.html",
    "title": "Final Project",
    "section": "",
    "text": "This is coming down the pipeline. Check again later.",
    "crumbs": [
      "Final Project"
    ]
  },
  {
    "objectID": "blog.html",
    "href": "blog.html",
    "title": "",
    "section": "",
    "text": "Code"
  },
  {
    "objectID": "test.html",
    "href": "test.html",
    "title": "",
    "section": "",
    "text": "CodeShow All CodeHide All Code\n\n\n\n\n\n\nCode\nimport pandas as pd\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nfrom sklearn.preprocessing import MultiLabelBinarizer\nfrom sklearn.model_selection import train_test_split, GridSearchCV\nfrom sklearn.multioutput import MultiOutputClassifier\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.metrics import hamming_loss, f1_score\n\n\n\n\nCode\ndata = pd.read_csv('BGG_Data_Set.csv', encoding='ISO-8859-1')\n# Replace NaN values with empty strings\ndata['Mechanics'] = data['Mechanics'].fillna('')\ndata['Domains'] = data['Domains'].fillna('')\n\n# Remove rows where 'Domains' is still empty\ndata = data[data['Domains'] != \"\"]\ndata = data.reset_index(drop=True)\n\n# Split strings into lists and clean them\ndata['Mechanics'] = data['Mechanics'].str.split(', ').apply(lambda x: [item.strip() for item in x if item.strip()])\ndata['Domains'] = data['Domains'].str.split(', ').apply(lambda x: [item.strip() for item in x if item.strip()])\n\nscaler = StandardScaler()\nscaled_numeric = scaler.fit_transform(data[['Min Players', 'Max Players', 'Play Time', 'Min Age', 'Rating Average', 'BGG Rank', 'Complexity Average']])\nscaled_numeric_df = pd.DataFrame(scaled_numeric, columns=['Min Players', 'Max Players', 'Play Time', 'Min Age', 'Rating Average', 'BGG Rank', 'Complexity Average'])\n\n# Binarize the cleaned lists\nmlb = MultiLabelBinarizer()\nattribute_matrix = pd.DataFrame(mlb.fit_transform(data['Mechanics']), columns=mlb.classes_)\ndomain_matrix = pd.DataFrame(mlb.fit_transform(data['Domains']), columns=mlb.classes_)\n\n# Build feature matrix from the same filtered `data`\nfeature_matrix = pd.concat([scaled_numeric_df, attribute_matrix], axis=1)\n\n\n\n\nCode\nx_train, x_test, y_train, y_test = train_test_split(feature_matrix, domain_matrix, test_size=0.2, random_state=42)\n\n\n\n\nCode\nknn = MultiOutputClassifier(KNeighborsClassifier())\n\nparam_grid_knn = {\n    'estimator__n_neighbors': [10,11,12],\n    'estimator__weights': ['uniform', 'distance'],\n    'estimator__metric': ['euclidean', 'manhattan']\n}\nknn_grid = GridSearchCV(knn, param_grid_knn, cv=5)\nknn_grid.fit(x_train, y_train)\nprint(\"Best KNN parameters:\", knn_grid.best_params_)\n\n\n\n\nCode\n\ny_pred = knn_grid.best_estimator_.predict(x_test)\naccuracy = accuracy_score(y_test, y_pred)\nprint(f\"KNN Accuracy: {accuracy}\")\nprint(\"Hamming Loss:\", hamming_loss(y_test, y_pred))\nprint(\"F1 Score (micro):\", f1_score(y_test, y_pred, average='micro'))\nprint(\"F1 Score (macro):\", f1_score(y_test, y_pred, average='macro'))\n\n\nKNN Accuracy: 0.6200294550810015\nHamming Loss: 0.0703240058910162\nF1 Score (micro): 0.729589428975932\nF1 Score (macro): 0.6247459019197275"
  },
  {
    "objectID": "blog.html#introduction",
    "href": "blog.html#introduction",
    "title": "",
    "section": "Introduction",
    "text": "Introduction\nHave you ever had a hobby that you wish you could predict, without having to do any of the work? Maybe you want to be able to predict what sport an athlete plays just based on height, weight, and eye color? Or guess which actor will be cast for a role based on their previous roles? In this tutorial, we’ll walk through how to use the K-Nearest Neighbors (KNN) algorithm to classify data based on the attributes they possess. We will use Board Game data as an example, modeling the type of board game based of attributes like player count, time, and complexity\nThis guide is perfect for Data Science students who want to apply machine learning to real-world data in a reproducible and interpretable way."
  },
  {
    "objectID": "blog.html#what-youll-learn",
    "href": "blog.html#what-youll-learn",
    "title": "",
    "section": "What You’ll Learn",
    "text": "What You’ll Learn\n\nHow to prepare data for modeling\nHow KNN works and why it’s useful for classification\nHow to implement KNN in Python using scikit-learn\nHow to evaluate model performance\nHow to interpret predictions"
  },
  {
    "objectID": "blog.html#step-1-understanding-the-data",
    "href": "blog.html#step-1-understanding-the-data",
    "title": "",
    "section": "Step 1: Understanding the Data",
    "text": "Step 1: Understanding the Data\nWe’ll use a simplified dataset of board games with the following attributes: ‘Min Players’, ‘Max Players’, ‘Play Time’, ‘Min Age’, ‘Rating Average’, ‘BGG Rank’, ‘Complexity Average’\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nName\nMin Players\nMax Players\nPlay Time\nMin Age\nRating Average\nBGG Rank\nComplexity Average\nType\n\n\n\n\nCodenames\n2\n8\n15\n10\n8.52\n20\n1.2\nParty\n\n\nDominion\n2\n4\n30\n13\n5.37\n5\n2.3\nCard Game\n\n\nTwilight Struggle\n2\n2\n180\n13\n2.80\n572\n3.8\nStrategy\n\n\n\n\nNote: Complexity is a rating from 1 (easy) to 5 (complex), sourced from BoardGameGeek\n\nK-Nearest Neighbors is a non-parametric, instance-based learning algorithm. It classifies a new data point based on the majority label of its k closest neighbors in the feature space. This means that if a new board game is entered and of its k closest neighbors, the majority are Strategy games, it will classify this new game as a Strategy game.\n\nThe KNN Formula\nTo compute the distance between two games, we use either Euclidean distance or Manhattan distance:\n\nEuclidean distance\n\\[\nd(x, y) = \\sqrt{\\sum_{i=1}^{n} (x_i - y_i)^2}\n\\] Where: - ( x ) and ( y ) are feature vectors - ( n ) is the number of features\n\n\nManhattan distance\n\\[\nd(x, y) = \\sum{i=1}^{n} |x_i - y_i|\n\\] Where: - ( x ) and ( y ) are feature vectors - ( n ) is the number of features\nWe will also choose how to weight our distances, either Uniform or Distance\nIf we choose Uniform, all of the neighbors will be weighted the same, while if we choose Distance, the neighbors that are closest will have the biggest influence."
  },
  {
    "objectID": "blog.html#step-2-implementing-knn-in-python",
    "href": "blog.html#step-2-implementing-knn-in-python",
    "title": "",
    "section": "Step 2: Implementing KNN in Python",
    "text": "Step 2: Implementing KNN in Python\nHere’s how to build a simple KNN classifier using scikit-learn.\n\nImport Libraries\nWe start by importing the necessary Libraries\n# Import libraries\nimport pandas as pd\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nfrom sklearn.preprocessing import MultiLabelBinarizer\nfrom sklearn.model_selection import train_test_split, GridSearchCV\nfrom sklearn.multioutput import MultiOutputClassifier\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.metrics import hamming_loss, f1_score\nThese librarys will allow us to access the functions we need to run this KNN algorithm.\n\n\nLoad and Prepare Data\nNext we will load and prepare the data, formatting it a way that allows us to run classification\ndata = pd.read_csv('BGG_Data_Set.csv', encoding='ISO-8859-1') #Replace the inside of the read_csv function with you file\nAfter we import the data we need to decide what to do with the null values, I chose to drop all of the data that wasn’t given a game type. You could also choose to replace null’s with the average or mode of the data, there are many different ways to deal with null data so you choose which works best for your data set\n# Replace NaN values with empty strings\ndata['Mechanics'] = data['Mechanics'].fillna('')\ndata['Domains'] = data['Domains'].fillna('')\n\n# Remove rows where 'Domains' is still empty\ndata = data[data['Domains'] != \"\"]\ndata = data.reset_index(drop=True)\nBecause my data has multiple types of games assigned to each game I had to split them up. First I separated each Domain and Mechanics with ‘,’ and then used “MultiLabelBinarizer” to separate them into different columns with 1 and 0 variables. I also standardized the data to make sure that no one attribute was pulling the algorithm too much. Finally I seperated the data into two data frames, one for the data I am using to train and one with the type of game for each game.\n# Split strings into lists and clean them\ndata['Mechanics'] = data['Mechanics'].str.split(', ').apply(lambda x: [item.strip() for item in x if item.strip()])\ndata['Domains'] = data['Domains'].str.split(', ').apply(lambda x: [item.strip() for item in x if item.strip()])\n\nscaler = StandardScaler()\nscaled_numeric = scaler.fit_transform(data[['Min Players', 'Max Players', 'Play Time', 'Min Age', 'Rating Average', 'BGG Rank', 'Complexity Average']])\nscaled_numeric_df = pd.DataFrame(scaled_numeric, columns=['Min Players', 'Max Players', 'Play Time', 'Min Age', 'Rating Average', 'BGG Rank', 'Complexity Average'])\n\n# Binarize the cleaned lists\nmlb = MultiLabelBinarizer()\nattribute_matrix = pd.DataFrame(mlb.fit_transform(data['Mechanics']), columns=mlb.classes_)\ndomain_matrix = pd.DataFrame(mlb.fit_transform(data['Domains']), columns=mlb.classes_)\n\n# Build feature matrix from the same filtered `data`\nfeature_matrix = pd.concat([scaled_numeric_df, attribute_matrix], axis=1)\n\n\nTrain/Fit Algorithm\nBefore you train your algorithm, you first need to split your data into test and train data. This function “train_test_split” takes a data frame of the attributes and a separate one of the “target” or answers and splits them up randomly.(the test_size parameter allows you to specify how much of the full data you want to be marked test data)\nx_train, x_test, y_train, y_test = train_test_split(feature_matrix, domain_matrix, test_size=0.2, random_state=42)\nThis next part is optional, but if you are not sure which parameters are the best for you data, you can run a grid search to test different combinations. This function will return the combination that performs the best(it may take a while). You can change the values for n_neighbors to values you think might be best. At the end of this function the “knn_grid.fit()” function will fit your model to the data, and now you have a working model\nknn = MultiOutputClassifier(KNeighborsClassifier())\n\nparam_grid_knn = {\n    'estimator__n_neighbors': [10,11,12],\n    'estimator__weights': ['uniform', 'distance'],\n    'estimator__metric': ['euclidean', 'manhattan']\n}\nknn_grid = GridSearchCV(knn, param_grid_knn, cv=5)\nknn_grid.fit(x_train, y_train)\nprint(\"Best KNN parameters:\", knn_grid.best_params_)\n\n\nTest Algorithm\nFinally it is time to test how well our model did. There are various different metrics to test your model and I will show you a few that you can use. Accuracy: This is just the percentage of correct guesses your alogithm made, a simple metric, but usefull useful Hamming loss: This is used specifically for multi-label classification, it measures the percent of labels that are incorrect(either a missed label or a wrong label). A lower score is better.\nF1 Score: This is a combination of both Precision(“Of all the instances my model predicted as positive, how many were actually positive?”) and Recall(“Of all the actual positive instances, how many did my model correctly identify?”)\ny_pred = knn_grid.best_estimator_.predict(x_test)\naccuracy = accuracy_score(y_test, y_pred)\nprint(f\"KNN Accuracy: {accuracy}\")\nprint(\"Hamming Loss:\", hamming_loss(y_test, y_pred))\nprint(\"F1 Score:\", f1_score(y_test, y_pred))\n\n\nUse Model\nNow that our model has been Trained, Fit, and Tested, we can use it on further data. By using the function\n\"model_name\".predict(\"new_data\")\nour model will give us its prediction for this new data point. Using our example we could predict what type a new game is, just based on its mechanical attributes."
  },
  {
    "objectID": "blog.html#conclusion",
    "href": "blog.html#conclusion",
    "title": "",
    "section": "Conclusion",
    "text": "Conclusion\nNow that you have learned how to implement a Multi-Label classification, KNN model, its time to go and try it yourself. Find yourself an interesting data set and see if you can create a KNN prediction model for it. Try out using different attributes or trying to predict different targets and see how the model fairs and use the accuracy scores to see how well you did. Once you are done with this you can go on to learn other prediction algorithms and continue to expand your data science toolkit!"
  },
  {
    "objectID": "about.html#experience-background",
    "href": "about.html#experience-background",
    "title": "About Me",
    "section": "Experience & Background",
    "text": "Experience & Background\n\nI am a senior at BYU majoring Data Science. I plan to graduate in 2027.\nI love working with data to find correlations and patterns in order to make predictions and informed decisions.\nI currently work for the BYU Statistics department as a grader and tutor for various data science courses, and as a research assistant for a professor in the department. In this role I help with the Learning Outcomes Assessment for the department, which involves collecting and analyzing data on student learning outcomes to help improve the curriculum and teaching methods."
  },
  {
    "objectID": "about.html#education-background",
    "href": "about.html#education-background",
    "title": "About Me",
    "section": "",
    "text": "[BS Data Science] - [BYU], [2027]\n[AS Mathmatics and Science] - [Santiago Canyon College], [2024]\nRelevant Coursework: Statistics(Both Frequentist and Bayesian), Data Analysis, Programming, Machine Learning, Data Visualization\n\n\n\n\n\n\n\nProgramming: Python, R, C++, SQL\nData Analysis: Pandas, NumPy\nVisualization: Matplotlib, Seaborn\nMachine Learning: Scikit-learn\nTools: Jupyter Notebooks, Git/GitHub, Rshiny\n\n\n\n\n\nI enjoy working with data in various domains, including:\n\nSports analytics(specifically Soccer)\nGaming(both online and physical games)\nEducation\n\n\n\n\n\n\n\nEmail: corbinpchristiansen@gmail.com\nGitHub: github.com/FBIagent42)\nLinkedIn: linkedin.com/in/corbin-christiansen\n\n\nThis portfolio showcases my learning progress and projects completed during my data science studies."
  },
  {
    "objectID": "eda_blog.html",
    "href": "eda_blog.html",
    "title": "",
    "section": "",
    "text": "Code"
  },
  {
    "objectID": "eda_blog.html#introduction",
    "href": "eda_blog.html#introduction",
    "title": "",
    "section": "Introduction",
    "text": "Introduction\nEvery couple of months, Wizards of the Coast releaces a new set of Magic the Gathering cards for player to buy and use. Each set has new cards with the set having its own unique focus, mechanics, and themes. Some sets are praised for being masterfuly created, containing fun and unique cards, and being a balenced set. Others crash and burn, with cretiques from players and generaly low popularity. These poorly built sets are a dissapointment for both players and Wizards of the Coast, and it would be very helpful if we could see what attributes a well built set poseses. If patterns can be found, Wizard would be able to consistantly make popular and fun sets for players to enjoy.\nIn this blog we will explore one particular set of card that is concedered to be one of the greatestes sets in the last couple of years; The Lord of the Rings set from June 2023. We want to see what the distribution of a confirmed popular and well built set looks like. We will explore different features of the set such as mana distribution, card types, or color distribution.\nOur main question for this data exploration is “What makes a good MTG set?”. We will compare the different features of the data to see if any pattern emerge. My expectation is that a well built set will have evenly distributed cards, with each color having a similar ammount of cards, the mana cost being centered around 3 with a tail for higher costing cards, and each color having card types that match its theme."
  },
  {
    "objectID": "eda_blog.html#data-collection",
    "href": "eda_blog.html#data-collection",
    "title": "",
    "section": "Data Collection",
    "text": "Data Collection\n\nEthical Practices\nTo gather this data I used a MTG API I found online. Because of the size of the data, multiple requests to the API were needed. As part of the documentation for the API, they throttle the number of requests you can submit within an hour to 5000. This is way more than we need(a total of 9 requests is all the code uses), but I also implimented a wait time of 1 second between each request to keep good scraping practices.\n\n\nCode for API\nThe first step for this particular API is creating the url. For this project we set up our code that analysis can be easily swapped from one set to another by changing the ‘endpoint’. If you would like to preform a similar analysis, you could change the three letter set code from ‘LTR’ to whichever set you would like to analize.\nbase_url = \"https://api.magicthegathering.io/v1/cards\"\nendpoint = \"?set=LTR\"\nurl = base_url + endpoint\nFrom there we made a simple loop to grab each page of the API, collecting the list of cards.\nall_cards = []\npage = 1\n\n#Loop though all pages of cards\nwhile True:\n    response = requests.get(url, params={\"page\": page})\n    data = response.json().get(\"cards\", [])\n    \n    if not data:  # Stop when no cards are returned\n        break\n    \n    #add cards to list\n    all_cards.extend(data)\n\n    #Output for number of pages\n    print(f\"Fetched page {page} with {len(data)} cards\")\n    page += 1\n\n    #Sleep time for good API etiquette\n    time.sleep(1)\n\n\nDuplicate Rows\nWe found a total of 856 cards by first filtering the API to only get cards in the Lord of the Rings set. Then we grabbed all the cards, iterating through each page, untill we scraped from every page available. One issue we found in the data was the number of duplicate cards, over half of the cards(about 500) we duplicate. This comes from the fact that each card has multiple different printings(art styles, teatments, languages, etc.), so we needed to condence this down to just one row per card. We ended up using only the original printing of each card, which brought the data set down to 289 cards."
  },
  {
    "objectID": "eda_blog.html#findings",
    "href": "eda_blog.html#findings",
    "title": "",
    "section": "Findings",
    "text": "Findings\nIn this blog we will first go throught the things we found that met our expectations and then we will discuss the findings that we did not expect.\n\nExpected Findings\n\nColor vs CMC\nWe found that the single colored cards had almost the exact same distribution as each other, with only the far tails differing slightly from each other. This is a good indicator of a balenced set, with each color having the same distribution of high costing and low costing cards.\n\n\n\nSingle Color Identity vs. Converted Mana Cost\n\n\nThe multi colored cards showed a bit more variation, but still seemed to hold similar distributions. One worthy note here is is the Black/Red cards (which are generaly very chaotic, random cards) have the largest range of mana values.\n\n\n\nMultiple Color Identity vs. Converted Mana Cost\n\n\n\n\nRarity vs CMC\nThe mana value of cards does not seem to differ from rarities, the mythic being the exception, but that is likely due to the fact that they are generaly more unique, powerful cards and therefor cost more.\n\n\n\nRarity vs. Converted Mana Cost\n\n\n\n\nCard Type vs CMC\nThe card types follow the general patterns for their respective types, with instants and artifacts on the lower side(faster paced cards made for the early game), Enchantments and Sorceries in the middle(basically the more powerful, later game versions of the latter), and Creatures having the highest mana value and also the widest distribution(these cards can range from fast early game cards, to game ending slow cards).\n\n\n\nCard Type vs. Converted Mana Cost\n\n\n\n\n\nUnexpected Findings\n\n\nDistribution of Colors\nWhat I found most interesting about the color distribution was the underwhelming number of multi colored cards. While it is standard to have less cards the more colors that are added, I was very supprised that not every combonation of color was represented in this set. I would expect a well received set to contain something for everyone and have not left out. This leads me to wonder if a balenced set is a good one.\n\n\n\nMono-color & Colorless\n\n\n\nColor Identity\nCount\n\n\n\n\n[W]\n42\n\n\n[B]\n42\n\n\n[R]\n42\n\n\n[G]\n41\n\n\n[U]\n40\n\n\nColorless\n19\n\n\n\n\n\nTwo-color\n\n\n\nColor Identity\nCount\n\n\n\n\n[G, W]\n6\n\n\n[B, R]\n6\n\n\n[B, W]\n5\n\n\n[G, U]\n4\n\n\n[G, R]\n4\n\n\n[R, U]\n4\n\n\n[B, U]\n4\n\n\n[U, W]\n4\n\n\n[R, W]\n4\n\n\n[B, G]\n4\n\n\n\n\n\nMulti-color\n\n\n\nColor Identity\nCount\n\n\n\n\n[G, R, U, W]\n1\n\n\n[B, U, W]\n1\n\n\n[B, R, U]\n1\n\n\n[B, G, R, U, W]\n1\n\n\n\n\n\n\n\nDistribution of Supertypes\nThis set had a large number of Legendary cards, about a third of the set, which I do not believe is normal for a MTG set. Players are often upset when good cards are legendary as it makes them harder to use, espessialy when a bulk of the set is so. I wonder if having to many legendary creatures is a bad thing, or if the other aspects of the set make up for it.\n\n\n\nSupertype\nCount\n\n\n\n\nNone\n175\n\n\n[Legendary]\n109\n\n\n[Basic]\n5\n\n\n\n\n\n\nDistribution of Card Type for each Color\nThis was the biggest upset in my findings. I was expecting each color to be the clear leader in its field (Green for creatures, Blue for Instants and Sorcerys, White for Enchantments, etc.) and i was blown away by the results. All of the colors were even across the board, with only Instanty and Sorcerys having a clear leader, and sometimes not the one expected. The only part of this that I was expecting was Colorless to have the most Artifacts, as it is the main theme of that color. Very facinating to see.\n\n\n\nCard Type by Color"
  },
  {
    "objectID": "eda_blog.html#conclusion",
    "href": "eda_blog.html#conclusion",
    "title": "",
    "section": "Conclusion",
    "text": "Conclusion\nWhile more research and analysis is required to make concreate judgments, it is interesting to see the trends of card in this set. I would encourage you readers to go and use this API for yourself on a different set and see if the findings are different. Are these trends the same for good sets, and different for bad sets, or is the popularity linked to a different metric and not found here. If you happen to find any corelations, please reach out as I would be delighted to hear about your discoveries.\nYou can find the API for yourself here and see the code I used to generate these graphs at my Github. This repository also has more in depth analysis that was not covered in this blog. Good luck with the web scraping and I hope this little example helped!"
  }
]